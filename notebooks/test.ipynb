{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Clustering Prototype\n",
    "\n",
    "## Step 1: Test Installation\n",
    "\n",
    "In this step, we'll verify that all necessary dependencies are installed and working. We'll import key libraries, check their versions, load a spaCy model, and run a simple language detection test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "\n",
    "# Print version information for troubleshooting\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "\n",
    "# Load spaCy's English model to verify it is installed correctly\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy model 'en_core_web_sm' loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading spaCy model:\", e)\n",
    "\n",
    "# Test language detection on some sample text\n",
    "sample_text = \"Hello World\"\n",
    "detected_language = detect(sample_text)\n",
    "print(f\"Language detection test on '{sample_text}': {detected_language} detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Perform Initial Cleaning of Raw Data\n",
    "\n",
    "In this step, we run our pre-processing script to clean up the raw input data. The script produces an output file in the `data/processed/` folder, which we'll use in subsequent steps. After running the script, we load the cleaned data and calculate some basic metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pre-processing script that cleans the raw data\n",
    "%run ../scripts/preprocess_cleaning.py\n",
    "\n",
    "# Load the cleaned data\n",
    "df = pd.read_csv('../data/processed/cleaned_data_example.csv')\n",
    "\n",
    "# Calculate metrics\n",
    "total_cases = len(df)\n",
    "cases_with_missing = df[['text 1 missing', 'text 2 missing', 'text 3 missing']] \\\n",
    "    .apply(lambda row: 'Y' in row.values, axis=1).sum()\n",
    "\n",
    "print(\"Initial Data Cleaning Metrics:\")\n",
    "print(f\"Total Cases Processed: {total_cases}\")\n",
    "print(f\"Cases with one or more missing text fields: {cases_with_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Perform Language Detection on Cleaned Data\n",
    "\n",
    "In this step, we take the cleaned data produced in Step 2 and run the language detection script. This script analyzes the \"all text\" field and adds a new column \"language\" with the detected language code. After running the script, we load the output and display the count of cases per detected language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the language detection script\n",
    "%run ../scripts/preprocess_langdetect.py\n",
    "\n",
    "# Load the output from language detection\n",
    "df = pd.read_csv('../data/processed/langdetect_data_example.csv')\n",
    "\n",
    "# Calculate and display metrics for detected languages\n",
    "language_counts = df['language'].value_counts()\n",
    "print(\"Detected Languages and Case Counts:\")\n",
    "for lang, count in language_counts.items():\n",
    "    print(f\"{lang}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Use Helsinki NLP to Translate Non-English Fields into English\n",
    "\n",
    "In this step, we run our Helsinki NLP script which:\n",
    "- Checks the detected language from Step 3.\n",
    "- Downloads the appropriate Helsinki translation pipeline if needed.\n",
    "- Translates non-English text from the \"all text\" column into English.\n",
    "- Saves the translated text in a new column for easier downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Helsinki NLP translation script\n",
    "%run ../scripts/preprocess_helsinki_nlp.py\n",
    "\n",
    "# Load the output CSV with the translated text\n",
    "df_translated = pd.read_csv('../data/processed/translated_data_example.csv')\n",
    "\n",
    "# Print metrics to verify translation results\n",
    "total_cases = len(df_translated)\n",
    "translated_cases = df_translated[df_translated['language'] != 'en'].shape[0]\n",
    "# Count how many rows have a different value in 'translated_text' compared to the original 'all text'\n",
    "changed_text_count = (df_translated['translated_text'] != df_translated['all text']).sum()\n",
    "\n",
    "print(\"Translation Metrics:\")\n",
    "print(f\"Total Cases Processed: {total_cases}\")\n",
    "print(f\"Cases with non-English text (translated): {translated_cases}\")\n",
    "print(f\"Cases where translation modified the text: {changed_text_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Use SpaCy to Tokenize the English Text\n",
    "\n",
    "In this step, we run our SpaCy tokenization script which performs the following:\n",
    "- Loads the translated English text.\n",
    "- Tokenizes and processes the text (e.g., removing punctuation, lowercasing, etc.).\n",
    "- Prepares the text for clustering by generating token IDs or embeddings.\n",
    "This is the final data preparation step before we proceed to clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read the output from the previous step\n",
      "spaCy model loaded successfully!\n",
      "Tokenization complete.\n",
      "Tokenization Metrics:\n",
      "Total Cases Processed: 30\n",
      "Tokenized Case Sample:, i do n't know where to find a feature in the mobile app directed customer to the navigation menu customer confirmed feature found\n"
     ]
    }
   ],
   "source": [
    "# Run the SpaCy tokenization script\n",
    "%run ../scripts/nlp_tokenizing.py\n",
    "\n",
    "# Load the output CSV with the tokenized text\n",
    "df_tokenized = pd.read_csv('../data/processed/tokenized_data_example.csv')\n",
    "\n",
    "# Print metrics to verify tokenization results\n",
    "total_cases = len(df_tokenized)\n",
    "first_translated_case = df_tokenized['translated_text'].iloc[0]\n",
    "first_tokenized_case = df_tokenized['tokenized_text'].iloc[0]\n",
    "\n",
    "print(\"Tokenization Metrics:\")\n",
    "print(f\"Total Cases Processed: {total_cases}\")\n",
    "print(f\"Input Sample:, {first_translated_case}\")\n",
    "print(f\"Output Sample:, {first_tokenized_case}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-clustering-prototype",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
